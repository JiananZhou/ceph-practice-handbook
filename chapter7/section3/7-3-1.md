#### 7.3.1 server端配置

1、安装tgt

```
apt-get install tgt
```

2、确认tgt支持rbd

```
# tgtadm --lld iscsi --op show --mode system | grep rbd
    rbd (bsoflags sync:direct)
```

以上输出表示支持。

3、创建一个image

```
ceph osd pool create iscsi-pool 32
rbd create iscsi-pool/image1 --size 10240 --image-format 2
```

4、在tgt服务中注册刚创建好的image

在已有 `/etc/tgt/targets.conf ` 中或者新建 `etc/tgt/conf.d/ceph.conf` 文件，并添加以下信息：

```
<target iqn.2017-03.rbd.test.com:iscsi>
    driver iscsi
    bs-type rbd
    backing-store iscsi-pool/image1
</target>
```

5、重启tgt服务

```
sevice tgt restart
```

#### 7.3.2 client端配置

1、安装open-scsi

```
apt-get install open-iscsi
```

2、启动open-scsi服务

```
service open-iscsi restart
```

3、发现目标设备

```
root@ceph-cli:~# iscsiadm -m discovery -t st -p 172.20.1.169
172.20.1.169:3260,1 iqn.2017-03.rbd.test.com:iscsi
root@ceph-cli:~# iscsiadm -m discovery -t st -p 172.20.1.170
172.20.1.170:3260,1 iqn.2017-03.rbd.test.com:iscsi
root@ceph-cli:~# iscsiadm -m discovery -t st -p 172.20.1.171
172.20.1.171:3260,1 iqn.2017-03.rbd.test.com:iscsi
```

4、登录

```
root@ceph-cli:~# iscsiadm -m node --login
Logging in to [iface: default, target: iqn.2017-03.rbd.test.com:iscsi, portal: 172.20.1.171,3260] (multiple)
Logging in to [iface: default, target: iqn.2017-03.rbd.test.com:iscsi, portal: 172.20.1.169,3260] (multiple)
Logging in to [iface: default, target: iqn.2017-03.rbd.test.com:iscsi, portal: 172.20.1.170,3260] (multiple)
Login to [iface: default, target: iqn.2017-03.rbd.test.com:iscsi, portal: 172.20.1.171,3260] successful.
Login to [iface: default, target: iqn.2017-03.rbd.test.com:iscsi, portal: 172.20.1.169,3260] successful.
Login to [iface: default, target: iqn.2017-03.rbd.test.com:iscsi, portal: 172.20.1.170,3260] successful.
```

查询已登录目标节点的会话

```
root@ceph-cli:~# iscsiadm -m session
tcp: [1] 172.20.1.169:3260,1 iqn.2017-02.com.yhc:rbdtarget
tcp: [2] 172.20.1.171:3260,1 iqn.2017-03.rbd.test.com:iscsi
tcp: [3] 172.20.1.169:3260,1 iqn.2017-03.rbd.test.com:iscsi
tcp: [4] 172.20.1.170:3260,1 iqn.2017-03.rbd.test.com:iscsi
```

此时系统上有多个磁盘的状态和容量都是一样的，其实它们是指向同一个 iSCSI 目标存储，只是访问的路径不同而已。

5、确认设备已挂载

```
root@ceph-cli:~# lsblk
NAME                       MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINT
sda                          8:0    0    10G  0 disk  
sdb                          8:16   0    10G  0 disk  
└─33000000100000001 (dm-0) 252:0    0    10G  0 mpath 
sdc                          8:32   0    10G  0 disk  
└─33000000100000001 (dm-0) 252:0    0    10G  0 mpath 
sdd                          8:48   0    10G  0 disk  
└─33000000100000001 (dm-0) 252:0    0    10G  0 mpath 
vda                        253:0    0    20G  0 disk  
├─vda1                     253:1    0     2M  0 part  
├─vda2                     253:2    0   476M  0 part  /boot
└─vda3                     253:3    0  19.5G  0 part  /
```

其中sd开头的就是iscsi设备，sdb、sdc、sdd就是上面刚挂载的那3个。sda是用linux-bio形式挂载的target。

6、查看挂载

用命令 `lsscsi` 查看target端的lun映射的块设备

```
root@ceph-cli:~# lsscsi
[2:0:0:0]    disk    SCST_BIO rbd0              320  /dev/sda 
[3:0:0:0]    storage IET      Controller       0001  -        
[3:0:0:1]    disk    IET      VIRTUAL-DISK     0001  /dev/sdc 
[4:0:0:0]    storage IET      Controller       0001  -        
[4:0:0:1]    disk    IET      VIRTUAL-DISK     0001  /dev/sdb 
[5:0:0:0]    storage IET      Controller       0001  -        
[5:0:0:1]    disk    IET      VIRTUAL-DISK     0001  /dev/sdd 
```

#### 